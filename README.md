# An√°lisis de ECGs con Grandes Modelos de Lenguaje: Estudio y Aplicaci√≥n de ICL

## üìã Descripci√≥n del Proyecto

Este proyecto investiga la aplicaci√≥n de **In-Context Learning (ICL)** para el an√°lisis de electrocardiogramas (ECG) utilizando modelos de lenguaje multimodales de c√≥digo abierto. El objetivo principal es evaluar hasta qu√© punto los modelos open-source pueden adaptarse a tareas de interpretaci√≥n de ECG sin acceso previo a datos de esta distribuci√≥n espec√≠fica, manteniendo la privacidad de datos m√©dicos sensibles mediante despliegue local.

### Motivaci√≥n

Los modelos propietarios como GPT-4o demuestran un buen rendimiento en tareas de interpretaci√≥n de ECG, probablemente debido a exposici√≥n durante el entrenamiento. Sin embargo, su uso requiere enviar datos m√©dicos sensibles a servidores externos, lo cual plantea serias preocupaciones de privacidad que los hospitales no est√°n dispuestos a aceptar.

**Este trabajo explora alternativas de c√≥digo abierto que pueden ejecutarse localmente**, protegiendo la privacidad de los datos mientras se investigan t√©cnicas para compensar la falta de exposici√≥n previa a ECGs.

## üéØ Hip√≥tesis de Investigaci√≥n

### Premisa Principal
El rendimiento de ICL se degrada significativamente cuando la distribuci√≥n de datos difiere sustancialmente de los datos de entrenamiento del modelo.

### Hip√≥tesis a Validar
1. **ICL solo**: Los modelos open-source sin exposici√≥n previa a ECGs mostrar√°n bajo rendimiento en tareas de interpretaci√≥n
2. **ICL + CBM**: Introducir razonamiento expl√≠cito mediante Concept Bottleneck Models puede mejorar interpretabilidad y rendimiento
3. **Fine-tuning conceptual**: Un ajuste fino enfocado √∫nicamente en conceptos b√°sicos de ECG (ondas P, Q, R, S, T y escala) sin etiquetas diagn√≥sticas puede habilitar el aprendizaje efectivo mediante ICL
4. **Recuperaci√≥n post-fine-tuning**: El modelo ajustado deber√≠a recuperar el rendimiento en distribuciones previamente problem√°ticas

## üî¨ Pipeline Experimental

El proyecto sigue un pipeline de 6 etapas progresivas:

### Fase Preliminar: Toy Experiment
Validaci√≥n de la premisa usando una tarea simplificada (secuencias simb√≥licas) para verificar que:
- ICL falla cuando los datos no siguen patrones familiares
- Fine-tuning b√°sico resuelve esta limitaci√≥n

### Fase Principal: Experimentos con ECGs Reales

1. **Baseline: MedGemma + ICL**
   - Evaluaci√≥n del modelo base MedGemma con In-Context Learning
   - M√©tricas de rendimiento en tareas de clasificaci√≥n de ECG
   - An√°lisis de casos de fallo

2. **MedGemma + ICL + CBM**
   - Integraci√≥n de Concept Bottleneck Models
   - Razonamiento expl√≠cito sobre conceptos interpretables
   - Comparaci√≥n de interpretabilidad vs baseline

3. **Fine-tuning Conceptual**
   - Ajuste fino en conceptos b√°sicos de ECG **sin etiquetas diagn√≥sticas**
   - Enfoque: ondas (P, Q, R, S, T), intervalos, y escala del papel
   - Novedad: evitar ruido de etiquetas diagn√≥sticas en el ajuste

4. **Modelo Fine-tuned + ICL**
   - Evaluaci√≥n del modelo ajustado con ICL
   - Comparaci√≥n con baseline (etapa 1)

5. **Modelo Fine-tuned + ICL + CBM**
   - Combinaci√≥n completa de t√©cnicas
   - Evaluaci√≥n final de rendimiento e interpretabilidad

## üóÇÔ∏è Estructura del Proyecto


## üöÄ Gu√≠a de Uso

### 1. Instalaci√≥n

```bash
# Crear entorno conda
conda env create -f environment.yml
conda activate ecg-icl

# O usar pip
pip install -r requirements.txt
```

### 2. Toy Experiment (Validaci√≥n de Premisa)

#### Generar datos sint√©ticos
```bash
python scripts/generate_toy_dataset.py --n-test-samples 999 --n-ood-samples 300
```

Esto genera:
- 24 ejemplos ICL (8 por clase)
- 999 ejemplos de test in-distribution
- 300 ejemplos de test out-of-distribution

#### Evaluar ICL

**Nota**: Necesitas un servidor LLM local compatible con OpenAI API (ej: LM Studio, llama.cpp)

```bash
# Zero-shot
python scripts/evaluate.py --task 1 --n-shots 0

# Few-shot (4 ejemplos)
python scripts/evaluate.py --task 1 --n-shots 4

# Evaluar en datos OOD
python scripts/evaluate.py --task 1 --n-shots 4 --ood

# Con modelo espec√≠fico
python scripts/evaluate.py --task 1 --n-shots 8 --model-name "medgemma-2b"
```

### 3. Preprocesar Datos Reales (PTB-XL)

```bash
# Descargar y preprocesar PTB-XL
python scripts/preprocess_ptbxl.py --data_dir data/raw/PTBXL --output_dir data/processed/ptbxl
```

### 4. Experimentos Principales

#### ICL con MedGemma
```bash
python scripts/evaluate.py --config configs/medgemma_icls.yaml
```

#### Entrenar CBM
```bash
python scripts/train_cbm.py --config configs/cbm_config.yaml
```

#### Fine-tuning Conceptual
```bash
python scripts/finetune_model.py --config configs/medgemma_finetune.yaml
```

## üìä Datasets

### Toy Experiment
- **Tipo**: Secuencias simb√≥licas sint√©ticas
- **Alfabeto**: `{.|:_~}`
- **Tarea**: Clasificaci√≥n de frecuencia card√≠aca en 3 clases
- **Prop√≥sito**: Validar hip√≥tesis de ICL en entorno controlado

### PTB-XL
- **Tipo**: ECGs reales de 12 derivaciones
- **Muestras**: ~21,800 registros
- **Fuente**: [PhysioNet](https://physionet.org/content/ptb-xl/)
- **Tareas**: Clasificaci√≥n de diagn√≥sticos card√≠acos

*Ver `docs/datasets.md` para m√°s detalles*

## üîß Configuraci√≥n de Modelo Local

Para ejecutar los experimentos necesitas un servidor LLM local. Opciones recomendadas:

### LM Studio (Recomendado para principiantes)
1. Descargar [LM Studio](https://lmstudio.ai/)
2. Cargar un modelo (ej: Llama 3, Mistral, MedGemma)
3. Iniciar servidor local (por defecto: `http://127.0.0.1:1234/v1`)

### Experimentos Principales
*Secci√≥n en progreso - se actualizar√° con resultados*

## üìö Documentaci√≥n Adicional

- [Metodolog√≠a detallada](docs/methodology.md)
- [Plan de experimentos](docs/experiments_plan.md)
- [Consideraciones de privacidad y √©tica](docs/privacy_ethics.md)
- [Referencias](docs/references.md)

## üîê Privacidad y √âtica

Este proyecto prioriza la privacidad de datos m√©dicos:
- Todos los modelos pueden ejecutarse **completamente en local**
- No se requiere conexi√≥n a APIs externas
- Compatible con entornos hospitalarios con restricciones de seguridad
- Datos sensibles nunca salen del servidor local

Ver `docs/privacy_ethics.md` para consideraciones detalladas.


## üìÑ Licencia

*Por determinar*

## üìñ Citaci√≥n

*Por determinar*

## ü§ù Contribuciones

Este es un proyecto de investigaci√≥n acad√©mica (TFG). 

---

**√öltima actualizaci√≥n**: Noviembre 2025
